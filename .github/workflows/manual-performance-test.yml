name: Core Module Performance Test

on:
  workflow_dispatch:
    inputs:
      engine:
        description: 'Performance engine to use'
        required: true
        default: 'jmeter'
        type: choice
        options:
          - jmeter
          - gatling
          - hybrid
      protocol:
        description: 'Test protocol to run'
        required: true
        default: 'http'
        type: choice
        options:
          - http
          - graphql
          - jdbc
          - soap
          - mqtt
          - jms
          - all
      threads:
        description: 'Number of threads'
        required: true
        default: '10'
        type: number
      ramp_up_seconds:
        description: 'Ramp-up period in seconds'
        required: true
        default: '5'
        type: number
      duration_seconds:
        description: 'Test duration in seconds'
        required: true
        default: '30'
        type: number
      iterations:
        description: 'Number of iterations (0 for infinite)'
        required: false
        default: '0'
        type: number
      target_url:
        description: 'Target URL or endpoint (for HTTP/GraphQL/SOAP)'
        required: false
        type: string
      config_file:
        description: 'Path to YAML config file (relative to generic-performance-framework-tests/src/test/resources/configs)'
        required: false
        default: 'sample_config.yaml'
        type: string
      threshold_assertions:
        description: 'Enable performance threshold assertions'
        required: false
        default: false
        type: boolean
      upload_report:
        description: 'Upload report to GitHub Pages'
        required: true
        default: true
        type: boolean
      notify_slack:
        description: 'Send Slack notification with results'
        required: false
        default: false
        type: boolean

jobs:
  run-performance-test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up JDK 21
      uses: actions/setup-java@v4
      with:
        java-version: '21'
        distribution: 'temurin'
        cache: maven
    
    - name: Build core and tests modules
      run: mvn -B clean package -DskipTests
    
    - name: Set test parameters
      run: |
        echo "ENGINE=${{ github.event.inputs.engine }}" >> $GITHUB_ENV
        echo "PROTOCOL=${{ github.event.inputs.protocol }}" >> $GITHUB_ENV
        echo "THREADS=${{ github.event.inputs.threads }}" >> $GITHUB_ENV
        echo "RAMP_UP=${{ github.event.inputs.ramp_up_seconds }}" >> $GITHUB_ENV
        echo "DURATION=${{ github.event.inputs.duration_seconds }}" >> $GITHUB_ENV
        echo "ITERATIONS=${{ github.event.inputs.iterations }}" >> $GITHUB_ENV
        echo "TARGET_URL=${{ github.event.inputs.target_url }}" >> $GITHUB_ENV
        echo "CONFIG_FILE=${{ github.event.inputs.config_file }}" >> $GITHUB_ENV
        echo "THRESHOLD_ASSERTIONS=${{ github.event.inputs.threshold_assertions }}" >> $GITHUB_ENV
        
        # Extract repository info for report links
        REPO_OWNER=$(echo $GITHUB_REPOSITORY | cut -d '/' -f 1)
        REPO_NAME=$(echo $GITHUB_REPOSITORY | cut -d '/' -f 2)
        echo "REPO_OWNER=$REPO_OWNER" >> $GITHUB_ENV
        echo "REPO_NAME=$REPO_NAME" >> $GITHUB_ENV
        
        # Create timestamp for unique report folder
        echo "TIMESTAMP=$(date +%Y%m%d_%H%M%S)" >> $GITHUB_ENV
        
        # Set up results directory paths
        mkdir -p ./results/jtl
        mkdir -p ./results/html
        mkdir -p ./results/logs
        echo "RESULTS_DIR=$(pwd)/results" >> $GITHUB_ENV
    
    - name: Prepare test runner
      run: |
        # Create a simple Java test runner using the core module
        cat > RunTest.java << 'EOF'
        import io.ecs.config.TestConfiguration;
        import io.ecs.config.YamlConfig;
        import io.ecs.core.PerformanceTest;
        import io.ecs.engine.Engine;
        import io.ecs.engine.EngineFactory;
        import io.ecs.engine.Protocol;
        import io.ecs.model.ExecutionConfig;
        import io.ecs.model.TestResult;
        import io.ecs.report.ReportGenerator;
        
        import java.io.File;
        import java.nio.file.Paths;
        import java.time.LocalDateTime;
        import java.time.format.DateTimeFormatter;
        
        public class RunTest {
            public static void main(String[] args) {
                try {
                    String configFile = System.getenv("CONFIG_FILE");
                    String resultsDir = System.getenv("RESULTS_DIR");
                    String protocol = System.getenv("PROTOCOL");
                    String engine = System.getenv("ENGINE");
                    int threads = Integer.parseInt(System.getenv("THREADS"));
                    int rampUp = Integer.parseInt(System.getenv("RAMP_UP"));
                    int duration = Integer.parseInt(System.getenv("DURATION"));
                    
                    // Load test configuration from specified YAML file
                    String configPath = "generic-performance-framework-tests/src/test/resources/configs/" + configFile;
                    File configFileObj = new File(configPath);
                    if (!configFileObj.exists()) {
                        System.out.println("Config file not found at: " + configPath);
                        System.exit(1);
                    }
                    
                    System.out.println("Loading config from: " + configPath);
                    TestConfiguration config = YamlConfig.fromFile(configPath);
                    
                    // Override with workflow parameters
                    ExecutionConfig execConfig = new ExecutionConfig();
                    execConfig.setThreads(threads);
                    execConfig.setRampUpSeconds(rampUp);
                    execConfig.setDurationSeconds(duration);
                    
                    String targetUrl = System.getenv("TARGET_URL");
                    if (targetUrl != null && !targetUrl.isEmpty()) {
                        execConfig.setTargetUrl(targetUrl);
                    }
                    
                    // Create performance test with chosen engine
                    Engine testEngine = EngineFactory.createEngine(config, engine);
                    if (testEngine == null) {
                        System.out.println("Failed to create engine: " + engine);
                        System.exit(1);
                    }
                    
                    // Set up test parameters
                    PerformanceTest test = new PerformanceTest(config, testEngine);
                    test.setExecutionConfig(execConfig);
                    
                    // Run test and get results
                    System.out.println("Starting test with engine: " + engine);
                    TestResult result = test.execute();
                    
                    // Generate report
                    ReportGenerator reporter = new ReportGenerator(result);
                    String reportPath = resultsDir + "/html/" + protocol + "_" + 
                        LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
                    reporter.generateHtmlReport(reportPath);
                    
                    // Write JTL file
                    String jtlPath = resultsDir + "/jtl/" + protocol + "_" + 
                        LocalDateTime.now().format(DateTimeFormatter.ofPattern("yyyyMMdd_HHmmss"));
                    reporter.writeJtlFile(jtlPath + "/results.jtl");
                    
                    // Print summary
                    System.out.println("Test completed successfully!");
                    System.out.println("Total requests: " + result.getTotalRequests());
                    System.out.println("Errors: " + result.getErrorCount());
                    System.out.println("Success rate: " + result.getSuccessRate() + "%");
                    System.out.println("Average response time: " + result.getAverageResponseTime() + "ms");
                    
                    System.out.println("Results stored in:");
                    System.out.println("- JTL: " + jtlPath);
                    System.out.println("- HTML: " + reportPath);
                    
                    System.exit(0);
                } catch (Exception e) {
                    System.err.println("Error running test: " + e.getMessage());
                    e.printStackTrace();
                    System.exit(1);
                }
            }
        }
        EOF
        
        # Compile with dependencies
        javac -cp generic-performance-framework-core/target/generic-performance-framework-core-1.0.0.jar:generic-performance-framework-tests/target/generic-performance-framework-tests-1.0.0.jar RunTest.java
    
    - name: Execute performance test
      id: run_test
      run: |
        # Build classpath
        CLASSPATH=$(find generic-performance-framework-core/target -name "*.jar" | tr '\n' ':')
        CLASSPATH="$CLASSPATH:$(find generic-performance-framework-tests/target -name "*.jar" | tr '\n' ':')"
        CLASSPATH="$CLASSPATH:."
        
        echo "Running performance test with:"
        echo "- Engine: $ENGINE"
        echo "- Protocol: $PROTOCOL"
        echo "- Threads: $THREADS"
        echo "- Ramp-up: $RAMP_UP seconds"
        echo "- Duration: $DURATION seconds"
        echo "- Config: $CONFIG_FILE"
        
        if [ -n "$TARGET_URL" ]; then
          echo "- Target URL: $TARGET_URL"
        fi
        
        # Run the test and capture exit code
        java -cp "$CLASSPATH" RunTest 2>&1 | tee "$RESULTS_DIR/logs/test-run.log"
        TEST_EXIT_CODE=${PIPESTATUS[0]}
        
        # Set test result status
        if [ $TEST_EXIT_CODE -eq 0 ]; then
          echo "status=success" >> $GITHUB_OUTPUT
          echo "Test completed successfully ✅"
        else
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "Test completed with errors ❌"
        fi
        
        # Find the most recent results directory for reports
        JTL_DIR=$(find $RESULTS_DIR/jtl -type d -name "${PROTOCOL}_*" | sort | tail -n 1)
        HTML_DIR=$(find $RESULTS_DIR/html -type d -name "${PROTOCOL}_*" | sort | tail -n 1)
        
        echo "JTL_DIR=$JTL_DIR" >> $GITHUB_ENV
        echo "HTML_DIR=$HTML_DIR" >> $GITHUB_ENV
        
        # Extract basic statistics for summary from log file
        TOTAL_REQUESTS=$(grep "Total requests:" "$RESULTS_DIR/logs/test-run.log" | awk '{print $3}')
        ERROR_COUNT=$(grep "Errors:" "$RESULTS_DIR/logs/test-run.log" | awk '{print $2}')
        SUCCESS_RATE=$(grep "Success rate:" "$RESULTS_DIR/logs/test-run.log" | awk '{print $3}' | sed 's/%//')
        AVG_RESPONSE=$(grep "Average response time:" "$RESULTS_DIR/logs/test-run.log" | awk '{print $4}' | sed 's/ms//')
        
        echo "TOTAL_REQUESTS=$TOTAL_REQUESTS" >> $GITHUB_ENV
        echo "ERROR_COUNT=$ERROR_COUNT" >> $GITHUB_ENV
        echo "SUCCESS_RATE=$SUCCESS_RATE" >> $GITHUB_ENV
        echo "AVG_RESPONSE=$AVG_RESPONSE" >> $GITHUB_ENV
        
        exit $TEST_EXIT_CODE
      continue-on-error: true
    
    - name: Upload JTL results
      uses: actions/upload-artifact@v4
      with:
        name: jtl-results-${{ env.PROTOCOL }}-${{ env.TIMESTAMP }}
        path: ${{ env.JTL_DIR }}
        retention-days: 30
    
    - name: Upload HTML reports
      uses: actions/upload-artifact@v4
      with:
        name: html-reports-${{ env.PROTOCOL }}-${{ env.TIMESTAMP }}
        path: ${{ env.HTML_DIR }}
        retention-days: 30
    
    - name: Deploy HTML reports to GitHub Pages
      if: ${{ github.event.inputs.upload_report == 'true' }}
      uses: JamesIves/github-pages-deploy-action@v4
      with:
        folder: ${{ env.HTML_DIR }}
        branch: gh-pages
        target-folder: reports/${{ env.PROTOCOL }}_${{ env.TIMESTAMP }}
    
    - name: Create performance report summary
      run: |
        echo "## Performance Test Results 📊" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Add status badge
        if [ "${{ steps.run_test.outputs.status }}" == "success" ]; then
          echo "**Status:** ✅ Success" >> $GITHUB_STEP_SUMMARY
        else
          echo "**Status:** ❌ Failed" >> $GITHUB_STEP_SUMMARY
        fi
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Parameters table
        echo "### Test Parameters" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
        echo "| --- | --- |" >> $GITHUB_STEP_SUMMARY
        echo "| Protocol | $PROTOCOL |" >> $GITHUB_STEP_SUMMARY
        echo "| Threads | $THREADS |" >> $GITHUB_STEP_SUMMARY
        echo "| Ramp-up (seconds) | $RAMP_UP |" >> $GITHUB_STEP_SUMMARY
        echo "| Duration (seconds) | $DURATION |" >> $GITHUB_STEP_SUMMARY
        
        if [ "$ITERATIONS" -gt 0 ]; then
          echo "| Iterations | $ITERATIONS |" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -n "$TARGET_URL" ]; then
          echo "| Target URL | $TARGET_URL |" >> $GITHUB_STEP_SUMMARY
        fi
        
        if [ -n "$CUSTOM_CONFIG" ]; then
          echo "| Custom Config | $CUSTOM_CONFIG |" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "| Timestamp | $(basename $HTML_DIR) |" >> $GITHUB_STEP_SUMMARY
        
        # Stats table
        if [ -n "$TOTAL_REQUESTS" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "| --- | --- |" >> $GITHUB_STEP_SUMMARY
          echo "| Total Requests | $TOTAL_REQUESTS |" >> $GITHUB_STEP_SUMMARY
          echo "| Errors | $ERROR_COUNT |" >> $GITHUB_STEP_SUMMARY
          echo "| Success Rate | $SUCCESS_RATE% |" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Links
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Links" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ github.event.inputs.upload_report == 'true' }}" == "true" ]; then
          REPORT_BASENAME=$(basename $HTML_DIR)
          echo "📈 [View HTML Report](https://$REPO_OWNER.github.io/$REPO_NAME/reports/${REPORT_BASENAME}/index.html)" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "📦 [Download JTL Results](https://github.com/$REPO_OWNER/$REPO_NAME/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
        echo "📊 [Download HTML Report](https://github.com/$REPO_OWNER/$REPO_NAME/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
    
    - name: Send Slack notification
      if: ${{ github.event.inputs.notify_slack == 'true' }}
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ steps.run_test.outputs.status }}
        fields: repo,message,commit,author,action,eventName,workflow
        text: |
          Performance Test Results: ${{ steps.run_test.outputs.status == 'success' && 'Passed ✅' || 'Failed ❌' }}
          Protocol: ${{ env.PROTOCOL }}
          Threads: ${{ env.THREADS }}
          Duration: ${{ env.DURATION }}s
          Success Rate: ${{ env.SUCCESS_RATE }}%
          
          View full report: https://${{ env.REPO_OWNER }}.github.io/${{ env.REPO_NAME }}/reports/$(basename ${{ env.HTML_DIR }})/index.html
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
      continue-on-error: true
  
  historical-comparison:
    needs: run-performance-test
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.upload_report == 'true' }}
    steps:
    - uses: actions/checkout@v4
      with:
        ref: gh-pages
    
    - name: List historical test results
      run: |
        if [ -d "reports" ]; then
          echo "## Historical Performance Comparison" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Date | Protocol | Report Link |" >> $GITHUB_STEP_SUMMARY
          echo "| --- | --- | --- |" >> $GITHUB_STEP_SUMMARY
          
          # List the most recent 5 reports in reverse chronological order
          ls -ldt reports/*/ 2>/dev/null | head -5 | while read dir; do
            DIR_NAME=$(basename "$dir")
            PROTOCOL=$(echo $DIR_NAME | cut -d '_' -f 1)
            TIMESTAMP=$(echo $DIR_NAME | cut -d '_' -f 2-3)
            READABLE_DATE=$(date -d "${TIMESTAMP//_/ }" "+%Y-%m-%d %H:%M:%S" 2>/dev/null || echo "$TIMESTAMP")
            
            echo "| $READABLE_DATE | $PROTOCOL | [View Report](https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/reports/$DIR_NAME/index.html) |" >> $GITHUB_STEP_SUMMARY
          done
        else
          echo "No previous test results found for comparison." >> $GITHUB_STEP_SUMMARY
        fi



